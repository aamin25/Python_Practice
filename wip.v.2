#!python2.7
# -*- utf-8 -*-

#import os
#import pymssql
#from requests import request
#from urllib import urllib.request

import re
import sys
import json
import time
import pyodbc
import urllib2
import datetime

from bs4 import BeautifulSoup
from tweepy import OAuthHandler
from tweepy import Stream
from tweepy.streaming import StreamListener


#pass security information to variables  <-- all the keys should be read from a file and should be removed from the code
#aamin twitter access keys
#consumer_key="7jbj51PhiMzlA50SpM0KTgARB"
#consumer_secret="sTjKyEeUojCDG8ja7EY00uk6BwlUavCEzlocgrhoC6NsO8mKsL"
#access_key = "155989537-TomVnKsSP49FIICZxIXc98DDqGLbsTCqf0Oh1lJt"
#access_secret = "yBXY1XUZ2p1v1537oSboFl0byRXPaKAhUfKIPKdoLZOD4"

#Pritam Twitter Access Keys
consumer_key='jNIbWQEM3qu42dl3zNnkzeC6N'
consumer_secret='XyjEh3LsdntGFmOzvAezaFXdwCzCpzNsnwUmLacunkOXO9hoOL'
access_key = '777513474343612416-xfRiudHVA3eIyG1jg9Zw8GkubZbdCSw'
access_secret = 'a6qPPjDm3nrQ16WUOj772CYBi2Vk7tP2Db1BmzRSN9lz8'

#Connection Parameter
cnxn = pyodbc.connect('Driver={ODBC Driver 13 for SQL Server};Server=tcp:personaltwitter.database.windows.net;PORT=1433;Database=Personal_TwitterDB;Uid=personaltwitteradmin;Pwd=Personal_Password')
cursor = cnxn.cursor()

#Variable which will be used for filtration of the tweets
filter_words = ['#android']
filter_lang = ['en']

#PowerBI rest API URL which will be used to post live sentiment scores to steaming dataset
url = 'https://api.powerbi.com/beta/42dc8b0f-4759-4afe-9348-41952eeaf98b/datasets/a8cd5d1b-3ab7-4ba4-ba0e-69577582d43a/rows?key=Ta4e34Wmx%2FmKGefrvRLZaxUw3GsmdT7o7GK284wg4qNGPB8k415C5zP8IofK8mgpwixm5XhXEZPnaZD71ED%2F0w%3D%3D'

#use variables to access twitter
auth = OAuthHandler(consumer_key,consumer_secret)
auth.set_access_token(access_key, access_secret)

#create an class called 'customStreamListener'
class listener(StreamListener):
    def on_data(self, data):
        
#       use the json module to load the data for processing.
        all_data = json.loads(data)
    
#       Check if there's any data in tweet before processing.
        if 'text' in all_data:
            process_db(all_data)
            return True
        else:
            return True

    def on_error(self):
        print >> sys.stderr, 'Encountered error with status code:', status_code
        return True  # Don't kill the stream

    def on_timeout(self):
        print >> sys.stderr, 'Timeout...'
        return True  # Don't kill the stream

def filter_tweet(tweet):
    '''
    Function: to remove urls account mentions and hashtags from the tweet.
    Param text: The tweet which needs to be filtered
    Return: The filtered text
    Dependencies: re package (Regex)
    '''
    return  (re.sub(r"(?:\@|https?\://?|\#)\S+", "", tweet))


def process_db(all_data):
    '''
    Function: To post the data to database for historical analysis and fetching
    Param: all_data: the data which is to be posted in the database
    Return:
    Dependencies: pyodbc
    '''
#   Filter the required data from twitter stream
    tweet_raw = all_data['text']
    tweetid = all_data['id_str']
    retweet = all_data['retweeted']
    username = all_data['user']['screen_name']
    favorited = all_data['favorited']
    user_followers = all_data['user']['followers_count']
    user_friends = all_data['user']['friends_count']
    retweet_count  = all_data['retweet_count']

#   Use the datatime method to convert the tweet post date in datetime format for inserting into database.
    ts = time.mktime(datetime.datetime.strptime(all_data['created_at'], '%a %b %d %H:%M:%S +0000 %Y').timetuple())
    timestamp = datetime.datetime.fromtimestamp(ts)

#   Use the BeautifulSoup function to read the anchor tags from source data
    source = BeautifulSoup(all_data['source'])
    source_info = ''
    for anchor in source.find_all('a'):
        source_info = anchor.text

#   Use the filter_tweet to filter out url's, #tags and account mentions from the twitter text.
    tweet = filter_tweet(tweet_raw)

#   Use sentiment_analysis function to find out the sentiment of the tweet
    sentiment_array = sentiment_analysis(tweet)
    sentiment_score = sentiment_array[0]
    sentiment_mean = sentiment_array[1]

#   Insert the values into the database
    cursor.execute('insert into personal_twitter_schema.tweets_data_test '
                   '(tweetid, '
                   'retweet, '
                   'username, '
                   'favorited, '
                   'user_followers, '
                   'user_friends, '
                   'retweet_count, '
                   'sentiment_score, '
                   'sentiment_mean,'
                   'tweet_time,'
                   'source_info) values (?,?,?,?,?,?,?,?,?,?,?)',
                    tweetid, 
                    retweet, 
                    username, 
                    favorited, 
                    user_followers, 
                    user_friends, 
                    retweet_count, 
                    sentiment_score, 
                    sentiment_mean,
                    timestamp,
                    source_info)
                   
#   Commit the database entry made               
    cnxn.commit()
                   
#   Prepare the data for posting the data live in streaming dataset of PowerBI                
    data = '[{{ "Sentiment_Score": "{0}", "Sentiment_Mean": "{1}" }}]'.format(sentiment_score, sentiment_mean)

#   Use the post function to Post the data in PowerBI Straming dataset
    post(url, data)
    


def sentiment_analysis(tweet): #modify for productionization
    '''
    Function: Analyzes the sentiment of the text sent using sentiment140 api.
    Param: :text: Pass the pre parsed text to the function without any url or hashtags
    Return: Returns the Sentiment_score and sentiment mean of the text passed in list format
    Dependencies: urllib2 and json for
    '''
#   Use the regex method to filter out unecessary UTF-8 symbols from the tweet before sending it to sentiment API.
    tweet =  re.sub(r"([\'|,|:|\-])", r' ', tweet)  # it\'s "this"  -- filter tweet()

#   Prepare the data before sending it to sentiment API.
    a = {"data": [{"text": tweet, "query": filter_words[0]}]}
    
#   Use the JSON module to convert the data in json format which is the standard input of sentiment API.
    text = json.loads(json.dumps(str(a)))  
    
#   Use the urllib2 module to sent the request to sentiment API.
    response = urllib2.urlopen('http://www.sentiment140.com/api/bulkClassifyJson?aapid=aamin25@gmail.com', text) 
    
#   Read the response from the sentiment API.
    page = response.read()  # get the response


#   Use try or except to process the output
    try:
        sentiment_out_raw = json.loads(page)  # parse the result. The result is in JSON format
        sentiment_score = sentiment_out_raw['data'][0]['polarity']
    except Exception as e:
        print e
        print tweet
        sentiment_score = 2

#   On the basis of sentiment_score set the values of postive,negative and neutral values for the Tweet.
    if sentiment_score >= 3 :
        sentiment_mean = 'Positive'
    elif sentiment_score >= 2 :
        sentiment_mean = 'Neutral'
    elif sentiment_score <= 1 :
        sentiment_mean = 'Negative'
    else:
        print 'Invalid Sentiment Value'

#   Return the sentiment_score and sentiment_mean
    return [sentiment_score,sentiment_mean]


def post(url, data):
    '''
    Function: to post data to powerbi streaming dataset.
    Param: Powerbi url along with data which is to be transmitted
    Return: Returns the HTTP response code
    Dependencies: urllib2 and time packages
    '''
    try:
    # make HTTP POST request to Power BI REST API
        req = urllib2.Request(url, data)
        response = urllib2.urlopen(req)
        # print("POST request to Power BI with data:{0}".format(data))
        # print("Response: HTTP {0} {1}\n".format(response.getcode(), response.read()))
        return [response.getcode(), response.read()]

    except urllib2.HTTPError as e:
        return [e.reason, e.code]

    except urllib2.URLError as e:
        return [e.reason, e.code]

    except Exception as e:
        return [e.reason, e.code]

#Calling the stram listener calss using the auth credentials.
twitterStream = Stream(auth, listener())

#Using filter object the filter tweets on the basis of required #tags and languages
twitterStream.filter(track=['#android'],languages=['en'])
